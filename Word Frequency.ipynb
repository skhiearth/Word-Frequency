{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Frequency\n",
    "\n",
    "Analysing the word frequency in documents and books off Project Gutenberg.\n",
    "Ever heard about the Ziph's Law, a theory stating that the frequency of any word is inversely proportional to its rank in a frequency table? Well, there is a pretty famous [Vsauce video](https://www.youtube.com/watch?v=fCn8zs912OE) on it.\n",
    "\n",
    "GitHub Repository: https://github.com/skhiearth/Word-Frequency\n",
    "\n",
    "<img src=\"http://robslink.com/SAS/democd82/word_frequency.png\" \n",
    "alt=\"Word Frequency\" width=\"300\" height=\"200\" border=\"5\" /></a>\n",
    "\n",
    "#### The goal is to plot the most frequent words in any text document or book published on the website of Project Gutenberg, a library of over 60,000 free eBooks. The pipeline implemented can be easily extended to any free document or book published on the internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The steps we would be following are:\n",
    "1. Using the **requests** package in Python to get the raw data of the document.\n",
    "2. Parsing the data using the **BeautifulSoup** package.\n",
    "3. Using regex in **NLTK** to tokenize and removing stopwords in the document, keeping only relevant words.\n",
    "4. Plotting the word frequencies using **nltk.FreqDist** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the stopwords, stemmer and Word Lemmatizer\n",
    "stopword = nltk.corpus.stopwords.words('english') # Defining Stopwords\n",
    "ps = nltk.PorterStemmer() # Defining the Porter Stemmer\n",
    "wn = nltk.WordNetLemmatizer() # Defining the Word Net Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adfa', 'asdjfn', 'asdufi']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    '''Function to clean the raw text.'''\n",
    "    \n",
    "    text_nopunct = \"\".join([char.lower() for char in text if char not in string.punctuation])\n",
    "    token = re.split(\"\\W+\", text_nopunct)\n",
    "    text_nostopword = [word for word in token if word not in stopword]\n",
    "    # clean_text = [ps.stem(word) for word in text_nostopword] \n",
    "    # We use lemmatizing because of it's higher sophistication and we don't have a performance bottleneck\n",
    "    clean_text = [wn.lemmatize(word) for word in text_nostopword]\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_word_freq(path_to_document):\n",
    "    '''Function to fetch any document off Project Gutenberg and plot a Frequency Distribution Graph of the words.'''\n",
    "    \n",
    "    # Using the requests package in Python to get the raw data of the document\n",
    "    r_out = requests.get(path_to_document)\n",
    "    text = r_out.text\n",
    "    \n",
    "    # Create a BeautifulSoup object from the HTML\n",
    "    bf_out = BeautifulSoup(text, \"html5lib\")\n",
    "    \n",
    "    # Obtaining the text from the BeautifulSoup object\n",
    "    doc_text = bf_out.get_text()\n",
    "    \n",
    "    # Calling the clean_text function to clean the text\n",
    "    clean_text(doc_text)\n",
    "    \n",
    "    # Creating the frequency distribution and plotting it.\n",
    "    freqdist1 = nltk.FreqDist(words_ns)\n",
    "    freqdist1.plot(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
